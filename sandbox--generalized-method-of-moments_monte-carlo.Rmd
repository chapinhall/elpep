---
title: "Estimation of Tract-by-Year Sociodemographics with Census Data -- a Monte Carlo Study"
author: "Nick Mader"
date: "r format(Sys.time(), '%b/%d/%Y')"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(data.table)
library(momentfit)
```

# Model

Let $\theta_gt$ represent a given characteristic describing a population local to geography $g$ at time $t$. A motivating example is the number of children in families that qualify for Head Start in a given Census tract, for a given year.

Given available Census data, we establish a series of conditions that these $\theta_gt$s meet. 

## Moment Conditions

The following moment conditions anchor the estimation of tract-by-year estimates of our community characteristics of interest.

### Adding Up Across Geographies

A first moment condition arises from an adding up condition, where the $\theta_gt$s add up across geographies to match an aggregate geographic total at a given time. ACS 1-year data on PUMAs, which are larger geographic areas containing many Census tracts. Let $g$ index geographies ${1,...,G}$. Then,

$$\sum_{g=1,...,G} \theta_{gt} = X_{Gt}$$

### Averaging Across Years

A second set of moment conditions arise from averaging, where the $\theta_gt$s average across $s$ year spans to equal a reporting statistic that is averaged across time, for a given small geography. ACS 5-year data are reported for Census tracts and of course have $s=5$. Then,

$$\frac{1}{s}\sum_{i = t-s+1}^{t} \theta_{gi} = \bar{x}_{gt}$$

where $\bar{x}_{gt}$ represents $s$ year average data across a span whose final year is $t$.

### Following Conditional Transitions

A third set of moment conditions represent transitions between values in adjacent years. This posits that local trends are generally reflective of macroeconomic trends for households in similar economic and geographic circumstances. The basic monthly data for all households entering the Current Population Survey in a given year is used to examine their likelihood of transitioning to another status in the year ahead. A linear probability model is estimated from those transitions in order to provide transition parameters that are applicable to the community-level.

Suppose that individual transitions are given by the following linear model

$$x_{it+1} = \lambda x_{it} + \epsilon_{it}$$

where individual values are draws from their community $x_{it}=\theta_{gt}+\nu_i+\eta_{t}$. Thus:

$$\theta_{gt+1} + \nu_i + \eta_{t+1} = \lambda\theta_{gt} + \nu_i + \eta_{t} + \epsilon_{it}$$

and 

$$E[\theta_{gt+1}|\theta_{gt}] = \lambda E[\theta_{gt}]$$

We can use linear estimation of equation XX to obtain a consistent estimate of $\hat{lambda}$ to use in equation XX.




### Initial Conditions

While the "Conditional Transitions" moments connect each tract's estimate of a give year to the year before it, the initial year itself must be anchored to some additional condition. For this we 


## Identification

For $t=1,...,T$ and $G$ geographies, and a single indicators, there are $T \times G$ parameters to estimate.

Across the above conditions, there are:

* $T$ adding up conditions
* $G \times (T-s+1)$ average-across conditions
* $G \times (T-1)$ transition conditions

Assuming that $T\geq s$, the total number of conditions is $G \times (2T - s) + T \geq G \times T + T > G\times T$, so that our parameter set is overidentified by the available conditions.

In estimation, we thus use Generalized Method of Moments estimation, where weights are not drawn from the moment variation obtained from a first step, but rather directly from the data given that the variation in moments is available directly from sampling variation known of the ACS 1-year (adding up) and 5-year surveys (averaging across) and estimation error using the CPS.

<!-- /!\ In the transition moments, the variance of epsilon is at the individual level. That can be adjusted to apply to community averages, as in the variance calculations above. However, there needs to be more thinking about how to represent the fact that there is spatial heterogeneity in both intercepts (the nu) and slopes. Consider estimating a mixed effects model with the CPS. -->


# Data Generation

To test recovery of our statistical estimates, we design a Monte Carlo experiment loosely calibrated off of realistic counts.

```{r set parameters}
G <- 45
T <- 8
extra_init <- 10 
  # use this many time periods to set dynamics ahead of the years used for 
  # observation. This is, in effect, the "burn in" period for data setup.
stopifnot(T >= 5)
span_length <- 5
beta <- 0.8
se_sum   <- 0.05*G           # This is error added to the sum across geos, in year
se_avg   <- 0.05*span_length # This is error added to the average across years
se_trans <- 0.10             # This is error added to the predicted transition
```


```{r generate true data}
gen_true_data <- 
  function(
    G          = 45,   # Number of smaller geographies in the larger
    A          = 8,    # Number of time periods (A for anos) involved in analysis
    extra_init = 10,   # Added "burn-in" time periods part of generating data
    beta       = 0.8,  # Slope parameter governing transitions from one period's value to the next
    sd_trans   = 0.10, # std dev of unobservables of the transition equation
    seed       = NULL  # if requested, set a seed
) {
    
  if(!is.null(seed)) set.seed(seed)
    
  # Initialize eventual estimates object
  thetas_g.t <- NULL
  
  # Initialize estimates for t = 0
  theta_t <- rnorm(G)
  thetas_g.t <- theta_t
  
  # Loop across time 
  for (t in 1:(extra_init + A)) {
    theta_t <- beta*theta_t + rnorm(G, sd = sd_trans)
    thetas_g.t <- cbind(thetas_g.t, theta_t)
  }
  
  # Set data names
  thetas_g.t <-
    thetas_g.t %>% 
    as.data.frame() %>% 
    setnames(as.character(0:(extra_init+A)))
  
  # Keep only thetas within our sample window (i.e. discarding initialized periods)
  thetas_g.t <- 
    select(thetas_g.t, one_of(as.character((extra_init + 1):(extra_init + A))))
  
  # Reshape the data to long form
  thetas_gt <- 
    thetas_g.t %>%
    mutate(g = 1:G) %>%
    pivot_longer(cols = -g, # pivot down every column but the geographic indicator
                 names_to = "time",
                 values_to = "theta_gt") %>%
    mutate(time = as.numeric(time) - extra_init) %>%
    data.table()
  
  out <- 
    list(out = thetas_gt,
         params = c("G" = G,
                    "T" = T,
                    "extra_init" = extra_init,
                    "beta" = beta,
                    "sd_trans" = sd_trans,
                    "seed" = seed))
  
  return(out)
}
```

```{r small functions for forming moments}
make_sumup <- function(dt) {
  dt[j = .(x_Gt = sum(x_gt)),
                by = time]
}

make_avgx <- function(dt, span_length) {
  thetas_avgx <- NULL
  for (i in span_length:max(dt$time)) {
    thetas_avgx <- 
      bind_rows(thetas_avgx,
                dt[time %in% (i - span_length + 1):i,
                   .(time = i,
                     x_gs = mean(x_gt)),
                   by = g])
  }
  thetas_avgx
}
```


```{r add error to draws of observed data}
draw_obs_data <- 
  function(se_est,          # Standard error of each theta given sample variation
           span_length = 5, # Number of years that are averaged together
           seed = NULL,     # Set a seed if desired
           ...) {
    
    if(!is.null(seed)) set.seed(seed)
    
    # Generate true parameters
    data_gen <- gen_true_data(seed = seed, ...)
    thetas_gt_true <- data_gen$out
    
    # Add noise to true parameters to reflect true sample noise
    thetas_gt <- 
      thetas_gt_true %>% 
      mutate(x_gt = theta_gt + rnorm(n(), 
                                     sd = se_est))
    
    # Sum estimates to get year-level estimates
    thetas_sumup <- make_sumup(thetas_gt)
    
    # Calculate averages across span
    thetas_avgx <- make_avgx(thetas_gt, span_length)
    
    # Generate a long-form data.table object with index for use in moment
    # calculation operations
    ix_gt <- 
      copy(thetas_gt) %>% 
      rename(theta0 = theta_gt) %>% 
      .[j = theta0 := 0] 
    
    # Produce output
    out <- list(thetas_gt    = thetas_gt,
                thetas_sumup = thetas_sumup,
                thetas_avgx  = thetas_avgx,
                ix_gt        = ix_gt,
                params       = c(data_gen$params, "s" = span_length))
    return(out)
  }
```


```{r attempt to draw data}
full <- 
  draw_obs_data(se_est = 0.1)
full_noerr <- 
  draw_obs_data(se_est = 0.0)
toy  <- draw_obs_data(A = 2,
                      G = 2,
                      span_length = 2,
                      extra_init = 0,
                      se_est = 0.1)
```

```{r compare true vs noise-added parameters}
full$thetas_gt %>% 
  ggplot(aes(x = theta_gt,
             y = x_gt)) +
  geom_point() +
  geom_hline(yintercept = 0,
             color = "gray") +
  geom_vline(xintercept = 0,
             color = "gray") +
  geom_abline(color = "blue") +
  labs(x = "True parameters",
       y = "Parameters with noise added") +
  theme_minimal()
```

```{r visualize paths of true parameters across time}

g_sample <- sample(full$thetas_gt$g, 6)

full$thetas_gt %>% 
  melt(id.vars = c("g", "time")) %>% 
  .[g %in% g_sample & between(time, max(time)-full$params[["s"]] + 1, max(time))] %>% 
  ggplot(aes(x = time,
             y = value,
             color = factor(variable),
             group = factor(variable))) +
  geom_hline(data = full$thetas_avgx[g %in% g_sample & time == max(time)],
             aes(yintercept = x_gs),
             color = "orange") +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_color_discrete(name = "") +
  facet_wrap(~ g,
             ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom")

```


# Moment generation

R's [`momentfit` package](https://cran.r-project.org/web/packages/momentfit/vignettes/gmmS4.pdf) is used to implement our method of moments. In terminology of that package, Functions `fct` and `dfct` can be used to return vectors of moment conditions, and the gradient of those moments with respect to the parameters. The `momentModel()` syntax can be used to return solutions.

`momentfit` takes `theta0` as a vector of length $k$ of estimates parameters. Together with an expectation of $q$ moments, `momentfit` requires moment functions to return vectors of length $q$, and gradient functions to return matrices that are $q x k$ (i.e. rows corresponding to the moments, and columns corresponding to parameters).

Confirming the above, we have $q = T + G(T-s) + G(T-1)$ moment conditions, and $k = G \times T$ parameters.

Next, we organize generation of these moments and gradients by each type of moment condition, and then produce a final function to run and stack each.

## Useful Functions

Although the GMM function will work with thetas organized in a single (long) vector, certain operations will benefit from being able to reshape these values. We organize our vector as $\theta_{gt}: \theta_{11} ... \theta_{1T}, \theta_{21}... \theta_{2T}, ... ..., \theta_{G1} ... \theta_{GT}$. That is, time recycles faster than geography.

```{r reshape thetas as either matrix or data table}

# Using data.table: horizontally append values to a labeled frame and reshape
reshape_thetas_dt_g.t <- function(ix_gt, theta) {
  ix_gt[j = thetas := theta] %>% 
    dcast(g ~ time,
          value.var = thetas)
}
```


## Functions for Calculating Moments and Gradients

### Summing Up Moments

These moment conditions are derived from the fact that our estimated parameter counts can be added up to match statistics reported for larger geographies at each point in time. Each moment condition thus takes the form of $\{\Sigma_g\theta_{gt}-\bar{x_t} = 0\}_t$, for $t=1,...,T$.

The gradient has the shape $T \times (TG)$ where each row is a binary indicator for whether the corresponding $\theta_tg$ is included in the row sum. That is, $1$ if the $t$ is the same as the row, $0$ otherwise.

```{r functions generate sumup moments and gradients using data.table}
sumup_moment_dt <- function(theta, x) {
  x$ix_gt %>% 
    # Set the theta0 column to the status of the input theta estimates
    .[j = theta0 := theta] %>% 
    .[j = sum(theta0),
      by = time] %>% 
    # Pull the vector of summed values
    pull(V1) %>% 
    # Form the moment
    as.vector() - x$thetas_sumup$x_Gt
}

# The sum-up moments are a simple, linear function of the parameters,
# the gradient is constant with respect to both the current parameters and data
# Thus, we pre-calculate that gradient and simply return it when the gradient
# function is called.

# /!\ Need to think of a way to embed this within our structures, given that it
# obeys the particular structure of the data, but where we shouldn't need to 
# recalculate it. This will be particularly consequential when we go to production
# when we have many large geographies with different numbers of small geographies
# (though the time points should be consistent)

# We calculate the gradient long-form (which makes sense with the data.table format)
# and then transpose to get the qxk structure

sumup_grad_dt <- function(x) {
  A <- x$params[["T"]]
  x$ix_gt %>% 
    .[j = lapply(1:A, function(t) 1*(time == t))] %>% 
    t()
}

```


### Average Across Moments

These moment conditions are derived from the fact that our estimated parameter can be averaged within given time spans to match statistics reported for each smaller geography.

Each moment condition thus takes the form of $\{\frac{1}{s}\Sigma_{i=t-s+1}^{t}\theta_{gi} - \bar{x}_{gt} = 0\}_{gt}$. We have $G \times (T-s)$ of these moment conditions.

```{r generate the average across moment using data.table functions}
avgx_moment_dt <- function(theta, x) {
  
  A <- x$params[["T"]]
  s <- x$params[["s"]]
  
  # Start with the long structure for the gxt's
  avgs <- 
    x$ix_gt %>% 
    # Assign the `theta0` column to the input theta
    .[j = theta0 := theta] %>% 
    # Operate from the first full span through the final time
    .[j = lapply(s:A, 
                 function(i) sum(theta0[time %in% (i-s+1):i])),
      by = g] %>% 
    setnames(c("g", s:A)) %>% 
    melt(id.vars = c("g")) %>% 
    .[j = as.vector(value)] 
  
  (avgs / s) - x$thetas_avgx$x_gs
    # This outputs a vector that is recycles `g` faster than `t` (necessary info)
    # to ensure that the gradient has the same order of moments
}

# The gradient calculation is: for each row, defined by a given g and t, determine 
# whether those match the g, and the time-span related to t. We call those values 
# in by going through rows of the observed avgx data as a single index for our 
# lapply().
avgx_grad_dt <- function(x) {
  
  A <- x$params[["T"]]
  s <- x$params[["s"]]
  
  avgs <- x$thetas_avgx
  
  # Start with the long structure for the gxt's
  x$ix_gt %>% 
    # Use lapply to generate columns for each avgx moment condition, and 
    # create flags for each row that contributes to the same g, and time in the span
    .[j = lapply(1:nrow(avgs),
                 function(row_of_avg) {
                   
                   my_avg_row <- avgs[row_of_avg,]
                   my_time <- my_avg_row$time
                   1*((g == my_avg_row$g) & 
                        (time %in% 
                           (my_time - s + 1):my_time))
                   
                 })] %>% 
    as.matrix() %>% 
    t() / 
    # Finally, divide by the span when our output is at its simplest
    s
}


```

### Transition Moments

These moment conditions are derived from the fact that each $\theta_{gt}$ can be predicted by the previous value $\theta_{gt-1}$ via the regression $\theta_{gt} = \beta\theta_{gt-1}$ where estimates of $\{\beta\}$ have been obtained from external study -- e.g. the Current Population Survey.

Our moments have the form $\theta_{gt} - \beta\theta_{g-1t}=0$, and we have $G \times (T-1)$ of them. 


```{r}
trans_moment_dt <- function(theta, x) {
  beta <- x$params[["beta"]]
  x$ix_gt %>% 
    .[j = theta0 := theta] %>% 
    .[j = theta0_lag := shift(theta0, n = 1, type = "lag"),
      by = g] %>% 
    .[!is.na(theta0_lag)] %>% 
    pull(theta0 - beta*theta0_lag)
}

trans_grad_dt <- function(x) {
  A <- x$params[["T"]]
  ix_wide <- x$ix_gt[time != 1] %>% select(g, time) %>% t()
  x$ix_gt %>% 
    .[j = sapply(1:ncol(ix_wide),
                 function(i) {
                   ix <- ix_wide[, i]
                   (g == ix[["g"]]) * 
                     ((time ==  ix[["time"]]    ) - 
                      (time == (ix[["time"]] - 1)))
                   }
                 )
      ] %>% 
    t()*1
}

if (FALSE) {
  z <- trans_grad_dt(full$ix_gt)
  z[1:10, 1:10]
  sum(z)
    # Should be 0
}
```


### Make and Combine Moments

For an `x` input that follows the structure as described above, these functions combine all components needed to specify a GMM model for estimation.

```{r make moments}
collect_moments <- 
  function(theta, x) {
      c(sumup_moment_dt(theta = theta, x = x),
         avgx_moment_dt(theta = theta, x = x),
        trans_moment_dt(theta = theta, x = x))
    }
```

```{r}
collect_grad <- 
  function(theta, x) {
    rbind(sumup_grad_dt(x),
           avgx_grad_dt(x),
          trans_grad_dt(x))
  }
```


# Implement the Method

## Test Run with Toy Data

Am finding it uncertain how to use the `momentfit` package for estimation. My current uncertainty is how `g()` is supposed to be applied to our data structure type. Instead, am considering use of the `optim()` function given that it works with our setup of objective function and gradient. The required modification is that the objective function `fn` returns a scalar, and `gr` is a function to return the gradients.

We can adapt our setup by using a squared sum of the output of our current moment-generating function, and updating the gradient.

```{r}
my_setup <- full_noerr

theta0  <- with(my_setup$ix_gt,
                rep(0, n_distinct(g)*n_distinct(time)))

my_x    <- my_setup$ix_gt

moms <- function(theta0) {
  collect_moments(theta = theta0,
                  x = my_setup)
}

grad <- function(theta0) {
  collect_grad(theta = theta0,
               x = my_setup)
}

# /!\ add weights
obj <- function(theta0) {
  m <- moms(theta0)
  t(m) %*% m
}

# m is q x 1
# d is q x k
# result is k x 1
obj_grad <- function(theta0) {
  m <- moms(theta0)
  d <- grad(theta0)
  t(m) %*% d
}

system.time({
  out <- 
    optim(
      #par = my_setup$ix_gt$x_gt, #rep(-5, length(theta0)), 
      par = rep(0, length(theta0)),
      #par = theta0,
      #par = out$par,
      fn = obj,
      gr = obj_grad,
      #method = "BFGS" # "Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"
      method = "L-BFGS-B"
      )
  out
})
```


```{r check output}
cbind(estimated_pars = out$par, my_setup$ix_gt)

# Plot estimates against each other
my_setup$ix_gt %>% 
  select(-theta0) %>% 
  mutate(est  = out$par,
         init = theta0) %>% 
  ggplot(aes(x = x_gt,
             y = est)) + 
  geom_point(alpha = 0.5) + 
  geom_point(aes(y = init,
                 color = "Initialized Parameter Values"),
             alpha = 0.3) +
  geom_abline() +
  geom_smooth() +
  scale_color_manual(name = "",
                     values = "red") +
  labs(x = "True Parameters",
       y = "Estimated Parameters") +
  theme_minimal() +
  theme(legend.position = "bottom")



```



# Examine Output


